# -*- coding: utf-8 -*-
"""elon musk

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h7P4lYNcU4XXv6J39i5-vqgCXZCEkqQq

#Config
"""

no_of_class_labels = 2
no_of_rows = 10000
#embeddings
max_words = 40000 #from pretrained model
max_len = 100 #embedding size

"""#1.Scraping

Unfortunately due to limitations in academic access to twitter api (either 403 or 401 error), I was unable to scrape data myself.
Instead, I am using tweets that are not yet cleaned and not tagged.
"""

import pandas as pd

df = pd.read_csv('Tweets.csv', usecols=['text'])
df = df.head(no_of_rows)

"""##1B.Remove stopwords and data cleanup"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re

nltk.download('stopwords')
nltk.download('punkt')

df['pure_text'] = df['text']
df_errors =  df.loc[~df['text'].apply(lambda x: isinstance(x, str))]
print(df_errors)

df['text'] = df['text'].str.lower()
df = df.loc[df['text'].apply(lambda x: isinstance(x, str))]
df['text'] = df['text'].apply(word_tokenize)
#TODO: usuwanie linków
df['text'] = df['text'].apply(lambda words: [re.sub(r'[^\w\s]','',word) for word in words])#usuwanie znaków specjalnych

df['text'] = df['text'].apply(lambda words: [word for word in words if word != '' and word not in stopwords.words('english')])

df['text'] = df['text'].apply(lambda x: x if len(x) > 0 else None)
df.dropna(subset=['text'], inplace=True)

"""##1C.Embeddings

Here I use pretrained english model - glove 6B. 
I use embedding vectors that have 100 dimensions.
"""

!wget https://nlp.stanford.edu/data/glove.6B.zip

from zipfile import ZipFile
with ZipFile('glove.6B.zip', 'r') as z:
  z.extractall()

import pandas as pd
import numpy as np

import tensorflow as tf
import keras
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences, to_categorical
from keras.models import Sequential, Model
from keras import layers

from sklearn.model_selection import train_test_split

import gensim
from gensim.models import Word2Vec
from gensim.models import KeyedVectors
import gensim.downloader as api

from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot

from keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=max_words)

texts = df.text
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

data = pad_sequences(sequences, maxlen=max_len)

embeddings_index = dict()
f = open('./glove.6B.100d.txt')
for line in f:
	values = line.split()
	word = values[0]
	coefs = np.asarray(values[1:], dtype='float32')
	embeddings_index[word] = coefs
f.close()
print(f'Loaded {len(embeddings_index)} word vectors.')

word_model = gensim.models.Word2Vec(df['text'], min_count=1) 
word_vectors = word_model.wv
sentence_embeddings = [np.mean([word_vectors[word] for word in sentence if word in word_vectors], axis=0)
                       for sentence in df['text']]
df['embedding'] = sentence_embeddings

vocab = tokenizer.sequences_to_texts(texts)
vocab_size = len(tokenizer.word_index) + 1 # Add 1 cause 0 index is reserved

rows = []
embedding_matrix = np.zeros((vocab_size, 100))
for word, i in tokenizer.word_index.items():
	embedding_vector = embeddings_index.get(word)
	if embedding_vector is not None:
		embedding_matrix[i] = embedding_vector
		rows.append({"words":word, "vectors":embedding_vector, "token":i})

"""##1D.K-means

"""

from sklearn.cluster import KMeans
data_array = np.vstack(df['embedding'].tolist())

kmeans = KMeans(n_clusters=no_of_class_labels,max_iter=1000,random_state=True, n_init=50)
kmeans.fit(df['embedding'].tolist())
labels = kmeans.labels_
df['cluster'] = labels

positive_cluster_index = 1
positive_cluster_center = kmeans.cluster_centers_[positive_cluster_index]
negative_cluster_center = kmeans.cluster_centers_[1-positive_cluster_index]

"""#2. Classic ML

"""

labels = to_categorical(kmeans.labels_, num_classes=no_of_class_labels)
labels

X_train, X_test,  y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state = 42, stratify=labels)

"""##Model 1"""

!pip install plot_keras_history
from sklearn.metrics import confusion_matrix
from plot_keras_history import show_history, plot_history
import matplotlib.pyplot as plt

model1 = Sequential()

model1.add(layers.Input(shape=(max_len,), dtype='int32', name='review_input'))
model1.add(layers.Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=max_len, trainable=True))
model1.add(layers.Flatten())
model1.add(layers.Dense(units=100, activation='relu'))
model1.add(layers.Dense(units=32, activation='relu'))
model1.add(layers.Dense(units=2, activation='softmax'))
model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

print(model1.summary())
SVG(model_to_dot(model1).create(prog='dot', format='svg'))

history1 = model1.fit(X_train,
                               y_train,
                               epochs=5,
                               batch_size=32,
                               verbose=True,
                               validation_data=(X_test,y_test))

# Get the predicted probabilities for each class
y_pred_probs = model1.predict(X_test)

# Convert probabilities to class predictions
y_pred = np.argmax(y_pred_probs, axis=1)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=1), y_pred)

print(cm)

show_history(history1)
plot_history(history1)
plt.close()

"""##Model 2"""

model2 = Sequential()

model2.add(layers.Input(shape=(max_len,), dtype='int32', name='review_input'))
model2.add(layers.Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=max_len, trainable=True))
model2.add(layers.Flatten())
model2.add(layers.Dense(units=100, activation='relu'))
model2.add(layers.Dropout(rate=0.5))
model2.add(layers.Dense(units=64, activation='relu'))
model2.add(layers.Dropout(rate=0.5))
model2.add(layers.Dense(units=2, activation='softmax'))
model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history2 = model2.fit(X_train,
                               y_train,
                               epochs=5,
                               batch_size=32,
                               verbose=True,
                               validation_data=(X_test,y_test))

# Get the predicted probabilities for each class
y_pred_probs = model2.predict(X_test)

# Convert probabilities to class predictions
y_pred = np.argmax(y_pred_probs, axis=1)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=1), y_pred)

print(cm)

show_history(history2)
plot_history(history2)
plt.close()

"""##Model 3"""

model3 = Sequential()

model3.add(layers.Input(shape=(max_len,), dtype='int32', name='review_input'))
model3.add(layers.Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=max_len, trainable=True))
model3.add(layers.Conv1D(filters=32, kernel_size=5, activation='relu'))
model3.add(layers.GlobalMaxPooling1D())
model3.add(layers.Dense(units=64, activation='relu'))
model3.add(layers.Dropout(rate=0.5))
model3.add(layers.Dense(units=2, activation='softmax'))
model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history3 = model3.fit(X_train,
                               y_train,
                               epochs=5,
                               batch_size=32,
                               verbose=True,
                               validation_data=(X_test,y_test))

# Get the predicted probabilities for each class
y_pred_probs = model3.predict(X_test)

# Convert probabilities to class predictions
y_pred = np.argmax(y_pred_probs, axis=1)

# Compute the confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=1), y_pred)

print(cm)

show_history(history3)
plot_history(history3)
plt.close()

"""#Step 3"""

!pip install wandb

num_classes=2 
input_dim=max_len

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.model_selection import train_test_split
import wandb

#wandb.init(project='twitter_sentiment')

num_epochs = 5
batch_sizes = [16]
learning_rates = [0.001, 0.1]
num_layers = [1, 2]
dropout_rates = [0.2, 0.5]
activation_functions = ['relu', 'sigmoid']

num_unit = 32

best_model = None;
best_acc = 0;
for batch_size in batch_sizes:
    for learning_rate in learning_rates:
        for num_layer in num_layers:
              for dropout_rate in dropout_rates:
                  for activation_function in activation_functions:
                        # Initialize WandB run for each set of parameters
                        wandb.init(project='fine_tuning_project', name=f"experiment_lr_{learning_rate}_nl_{num_layer}_activ_{activation_function}_dropout_{dropout_rate}",)
                        
                        # Set the hyperparameters in the run
                        wandb.config.batch_size = batch_size
                        wandb.config.learning_rate = learning_rate
                        wandb.config.num_layers = num_layer
                        wandb.config.num_units = num_unit
                        wandb.config.dropout_rate = dropout_rate
                        wandb.config.activation_function = activation_function

                        model = Sequential()
                        model.add(layers.Input(shape=(max_len,), dtype='int32', name='review_input'))
                        model.add(layers.Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=max_len, trainable=True))
                        model.add(layers.Flatten())
                        model.add(Dense(units=num_unit, activation=activation_function, input_shape=(input_dim,)))
                        model.add(Dropout(dropout_rate))
                        for _ in range(num_layer-1):
                            model.add(Dense(units=num_unit, activation=activation_function))
                            model.add(Dropout(dropout_rate))
                        model.add(Dense(units=num_classes, activation='softmax'))
                        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
                        model.optimizer.lr.assign(learning_rate)

                        # Train and validate the model
                        history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=num_epochs, verbose=1)
                        val_accuracy = history.history['val_accuracy'][-1]

                        # Log metrics to WandB
                        wandb.log({'val_accuracy': val_accuracy})

                        if (best_model is None):
                          best_model = model
                          best_acc = val_accuracy
                        else:
                          if(val_accuracy>best_acc):
                            best_acc = val_accuracy
                            best_model = model

                        wandb.finish()  # Finish the WandB run

#save model
model.save('part3.model')

"""#Other"""

!pip install accelerate
!pip install transformers==4.28.0

df_for_bert = df.loc[:, ["pure_text", "cluster"]]
df_for_bert.columns = ["text","label"]

!pip install datasets

from datasets import Dataset
ds = Dataset.from_pandas(df_for_bert)
labels = to_categorical(df_for_bert.label,num_classes=2)
dataset = ds.train_test_split(test_size = 0.1)

from transformers import AutoTokenizer

def process(x):
  return tokenizer(x['text'])

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
train_ds = dataset['train'].map(process)
test_ds = dataset['test'].map(process)

from transformers import AutoModelForSequenceClassification, TrainingArguments , Trainer 

model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = 2)

args = TrainingArguments('distilbert-base-uncased',evaluation_strategy ='epoch',save_strategy ='epoch',learning_rate=2e-5, per_device_train_batch_size =batch_size, per_device_eval_batch_size =batch_size, num_train_epochs =5, 
                         weight_decay =0.01, load_best_model_at_end = True, metric_for_best_model = 'accuracy')

from datasets import load_metric
metric = load_metric('glue','sst2')

import numpy as np
def compute_metrics(eval_preds):
  logits, labels = eval_preds
  predictions = np.argmax(logits,axis = -1)
  return metric.compute(predictions = predictions, references = labels)

trainer = Trainer(model, args, train_dataset = train_ds, eval_dataset = test_ds, tokenizer = tokenizer, compute_metrics = compute_metrics)

trainer.evaluate([train_ds[0]])

trainer.train()

model.save_pretrained('part4.model')